Este vai ser um README diferente!

Primeiramente gostaria de agradeçer a oportunidade de fazer parte do processo.

###pRobot_Correio_Neoway

Vamos as especificações:

> - Selenium para automatizar a navegação.
> - Utilizei o Firefox como navegador.
> - Requests para as requisições url.
> - Utilizei o BeautifulSoup para a análise do html.

> - Utilizei o Pycharm como IDE.

> - Rodando o arquivo: precisará do arquivo "geckodriver.exe" (https://github.com/mozilla/geckodriver/releases) para iniciar o "robô" no firefox.
Copie o caminho do arquivo, abra o cmd e cole com o comando "cd". ex:"C:\Users\SeuPC>cd localDoArquivo".
Após isto, execute o programa com python. ex:"C:\Users\SeuPC\localDoArquivo>python pRobot_Correio_Neoway.py"

Sobre o arquivo "pRobot_Correio_Neoway", tive uma dificuldade sobre captar os dados das tabelas após iniciada a busca.
Acredito que alguma funcionalidade do JavaScript que dificultou a obtenção dos dados.
Por mais que obtivesse as url's das páginas das tabeas corretamente, não conseguia obter o html da página.

Infelizmente não consegui completar o desafio, mas estou estudando para corrigir esta minha falha.

###pRobot_BoxLoja_Neoway

Fiz um Scraping em um site também com dados de faixa de cep.
Obtive o conteúdo do html e tratrei os dados. 
Realizei todo o código no Google Colaboratory para ficar mais fácil de realizar a análise.
Deixei tudo o mais detalhado possível.
